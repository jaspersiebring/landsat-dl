{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## landsat-dl\n",
    "*01-11-2017 - Jasper Siebring*\n",
    "\n",
    "Tool that allows you to search and generate download URLs for LANDSAT imagery. Originally based on Olivier Hagolle' [LANDSAT-DOWNLOAD](https://github.com/olivierhagolle/LANDSAT-Download) tool but with added support for collection 1 data and overall improvements across the board (also written in Python 3 instead of 2). Uses the USGS api to get any selected imagery' storage directories on the USGS servers which is needed to generate a valid download URL. Storage directories for pre-collection datasets are still manually assessed and can sadly change at any time. Collection 1' directories however are extracted directly from their API making it much more reliable (also much faster). \n",
    "\n",
    "**Features**:\n",
    "- Query LANDSAT MSS/TM/7/8 imagery ([pre-collection and collection 1](https://landsat.usgs.gov/download-entire-collection-metadata))\n",
    "- Filter using path/row/date/cloudcover/day/night\n",
    "- Preview selected imagery (using preview image url found in metadata)  \n",
    "- Automatically downloads (and updates if needed) metadata required for querying\n",
    "- Generates download URLs for any selected LANDSAT imagery\n",
    "- Directly download from generated URLs (requires logging in to the USGS website) using urllibs\n",
    "- Checks validity of given dates and generated URLs\n",
    "- Written in Python 3.5\n",
    "\n",
    "**To be implemented**:\n",
    "- Download using the requests library (current download functionality is limited and unnecessarily slow)\n",
    "- Metadata query via some external database (preventing local downloading of large metadata files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##setup for Conda environment (not necessary but does provide easy package downloading)\n",
    "#if conda is not installed (or not added to the command prompt) you will have to download the required packages elsewhere\n",
    "\n",
    "#!conda create --name {env_name} python=3.5 jupyter\n",
    "#!activate {env_name}\n",
    "#!conda install usgs -c conda-forge\n",
    "#!conda install pandas pillow requests dateutil datetime csv calendar io urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, os, datetime, sys, csv\n",
    "from dateutil import parser\n",
    "import requests\n",
    "from usgs import api\n",
    "import pandas as pd \n",
    "from calendar import monthrange \n",
    "from PIL import Image \n",
    "from io import BytesIO \n",
    "from usgs import USGSError\n",
    "\n",
    "import urllib \n",
    "import urllib.request as depr_urllib2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login details for the USGS api and website\n",
    "usgs_details = {'username' : 'your_username', 'password' : 'your_password'}\n",
    "\n",
    "#     or enter your details in the given txt file and uncomment the following line\n",
    "#usgs_details = get_usgs_details(txt_location)\n",
    "\n",
    "api.login(usgs_details['username'], usgs_details['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#downloads or updates preexisting metadata\n",
    "def update_metadata(collection, end):\n",
    "    #destdir = collection, might be list\n",
    "    for destdir in collection: \n",
    "        if not os.path.exists(os.path.dirname(destdir)):\n",
    "            print(\"Metadata (sub)folder is not found, creating..\")\n",
    "            os.makedirs(os.path.dirname(destdir))\n",
    "\n",
    "        base_url = \"https://landsat.usgs.gov/landsat/metadata_service/bulk_metadata_files/\"\n",
    "        base = os.path.basename(destdir)\n",
    "\n",
    "        url = os.path.join(base_url, base)\n",
    "        if os.path.isfile(destdir):\n",
    "            print(\"Metadata already exists, checking if up-to-date..\")\n",
    "            last_updated = requests.head(url)\n",
    "            last_updated = parser.parse(last_updated.headers['Last-Modified']).date()\n",
    "            if end > last_updated:\n",
    "                print(\"Metadata not up-to-date, downloading..\")\n",
    "                download_file(url, destdir)\n",
    "            elif end < last_updated:\n",
    "                print(\"Metadata up-to-date for specified dates, cont..\")\n",
    "        else:    \n",
    "            print (\"Metadata not found, downloading {x} for the first time..\".format(x = base))\n",
    "            download_file(url, destdir)\n",
    "            \n",
    "#accepts both a list and single objects\n",
    "def check_dates(dt):\n",
    "    if (type(dt) == datetime.date or type(dt) == datetime.datetime):\n",
    "        dt = [dt]\n",
    "        \n",
    "    boo_list = []\n",
    "    for dt_obj in dt:\n",
    "        boo_list.append((dt_obj.day == monthrange(dt_obj.year, dt_obj.month)[1] or dt_obj.day in range(1, monthrange(dt_obj.year, dt_obj.month)[1])))\n",
    "    if len(boo_list) == 1:\n",
    "        return(boo_list[0])\n",
    "    return(boo_list)\n",
    "\n",
    "def find_in_collection(destdir, sat_path, sat_row, sat, arc_col = 1, start=None, end=None, slc = False):\n",
    "    \n",
    "    #start lt5 collection 1 appears to be from 1984 onwards\n",
    "    #pre-collection\n",
    "    pre_l8 = pd.date_range(start= datetime.datetime(2013, 4, 11).date(), end=datetime.datetime(2017, 4, 30).date()) \n",
    "    pre_l7_slc_on = pd.date_range(start= datetime.datetime(1999, 1, 1).date(), end=datetime.datetime(2003, 5, 31).date()) \n",
    "    pre_l7_slc_off = pd.date_range(start= datetime.datetime(2003, 6, 1).date(), end=datetime.datetime(2017, 4, 30).date())\n",
    "    pre_tm_1 = pd.date_range(start = datetime.datetime(1980, 1, 1).date(), end = datetime.datetime(1989, 12, 31).date())  \n",
    "    pre_tm_2 = pd.date_range(start = datetime.datetime(1990, 1, 1).date(), end = datetime.datetime(1999, 12, 31).date())\n",
    "    pre_tm_3 = pd.date_range(start = datetime.datetime(2000, 1, 1).date(), end = datetime.datetime(2009, 12, 31).date())\n",
    "    pre_tm_4 = pd.date_range(start = datetime.datetime(2010, 1, 1).date(), end = datetime.datetime(2013, 12, 31).date())\n",
    "    pre_mss1_1 = pd.date_range(start = datetime.datetime(1982, 1,1).date(), end = datetime.datetime(1997, 12, 31).date())\n",
    "    pre_mss1_2 = pd.date_range(start = datetime.datetime(2012, 1,1).date(), end = datetime.datetime(2013, 12, 31).date())\n",
    "    pre_mss2 = pd.date_range(start = datetime.datetime(1972, 1,1).date(), end = datetime.datetime(1983, 12, 31).date())\n",
    "\n",
    "    #collection 1\n",
    "    c1_l8 = pd.date_range(start= datetime.datetime(2013, 4, 11).date(), end=datetime.datetime.now().date()) \n",
    "    c1_l7 = pd.date_range(start= datetime.datetime(1999, 1, 1).date(), end=datetime.datetime.now().date()) \n",
    "    c1_tm = pd.date_range(start= datetime.datetime(1980, 1, 1).date(), end=datetime.datetime(2012, 12, 31).date())\n",
    "    \n",
    "    collection = []\n",
    "    metadest = os.path.join(destdir, \"metadata\")\n",
    "    \n",
    "    if start == None:\n",
    "        print(\"Startdate not given, reverting to launch of satellite {}\".format(str(sat)))\n",
    "        if sat == \"LT5\":\n",
    "            if arc_col == 1:\n",
    "                start = min(c1_tm).to_pydatetime().date()\n",
    "            if arc_col == 0:\n",
    "                start = min(pre_tm_1).to_pydatetime().date()\n",
    "                \n",
    "        if sat == \"LE7\":\n",
    "            if arc_col == 1:\n",
    "                start = min(c1_l7).to_pydatetime().date()\n",
    "            if arc_col == 0:\n",
    "                start = min(pre_l7_slc_on).to_pydatetime().date()\n",
    "                \n",
    "        if sat == \"LC8\":\n",
    "            if arc_col == 1:\n",
    "                start = min(c1_l8).to_pydatetime().date()\n",
    "            if arc_col == 0:\n",
    "                start = min(pre_l8).to_pydatetime().date()\n",
    "    \n",
    "    if end == None:\n",
    "        print(\"Enddate not given, reverting to today (or end of satellite/collection)\")\n",
    "        if sat == \"LT5\":\n",
    "            if arc_col == 1:\n",
    "                end = max(c1_tm).to_pydatetime().date()\n",
    "            if arc_col == 0:\n",
    "                end = max(pre_tm_4).to_pydatetime().date()\n",
    "        if sat == \"LE7\":\n",
    "            if arc_col == 1:\n",
    "                end = max(c1_l7).to_pydatetime().date()\n",
    "            if arc_col == 0:\n",
    "                end = max(pre_l7_slc_off).to_pydatetime().date() \n",
    "        if sat == \"LC8\":\n",
    "            if arc_col == 1:\n",
    "                end = datetime.datetime.now().date()\n",
    "            if arc_col == 0:\n",
    "                end = max(pre_l8).to_pydatetime().date()\n",
    "    \n",
    "    if not (check_dates(start) and check_dates(end)):\n",
    "        print(\"One of the given dates doesn't exist, exiting..\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    ##################################\n",
    "    ##pre-collection\n",
    "    if arc_col == 0:\n",
    "        subfolder = \"pre_collection\"\n",
    "        if sat == \"LC8\": \n",
    "            if (start in pre_l8 or end in pre_l8):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_8.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_8.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "                \n",
    "\n",
    "        if sat == \"LE7\":\n",
    "            if (start in pre_l7_slc_on or end in pre_l7_slc_on):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_ETM.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_ETM.csv\"))\n",
    "            \n",
    "            elif (start in pre_l7_slc_off or end in pre_l7_slc_off):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_ETM_SLC_OFF.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_ETM_SLC_OFF.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "                #raise ValueError(\"Given dates are outside satellite collection..\")\n",
    "\n",
    "                    \n",
    "        if sat == \"LT5\":\n",
    "            if (start in pre_tm_1 or end in pre_tm_1):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_TM-1980-1989.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_TM-1980-1989.csv\"))\n",
    "            \n",
    "            elif (start in pre_tm_2 or end in pre_tm_2):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_TM-1990-1999.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_TM-1990-1999.csv\"))\n",
    "            \n",
    "            elif (start in pre_tm_3 or end in pre_tm_3):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_TM-2000-2009.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_TM-2000-2009.csv\"))\n",
    "            \n",
    "            elif (start in pre_tm_4 or end in pre_tm_4):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_TM-2010-2012.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_TM-2010-2012.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "        \n",
    "        if sat == \"MSS1\":\n",
    "            if (start in pre_mss1_1 or end in pre_mss1_1 or start in pre_mss1_2 or end in pre_mss1_2):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_MSS1.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_MSS1.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "        \n",
    "        if sat == \"MSS2\":\n",
    "            if (start in pre_mss2 or end in pre_mss2):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_MSS2.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_MSS2.csv\"))          \n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "                \n",
    "    #defaults to 1                            \n",
    "    if arc_col == 1:\n",
    "        subfolder = \"collection_1\"\n",
    "        if sat == \"LC8\":\n",
    "            if (start in c1_l8 or end in c1_l8):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_8_C1.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_8_C1.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "                \n",
    "        if sat == \"LE7\":\n",
    "            if (start in c1_l7 or end in c1_l7):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_ETM_C1.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_ETM_C1.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "                \n",
    "        if sat == \"LT5\":\n",
    "            if (start in c1_tm or end in c1_tm):\n",
    "                if not os.path.join(metadest, subfolder, \"LANDSAT_TM_C1.csv\") in collection:\n",
    "                    collection.append(os.path.join(metadest, subfolder, \"LANDSAT_TM_C1.csv\"))\n",
    "            else:\n",
    "                print(\"Both dates not in satellite collection(s)..\")\n",
    "        \n",
    "    #updates required metadata files\n",
    "    update_metadata(collection, end)\n",
    "\n",
    "    files = []\n",
    "    for col in collection:\n",
    "        print(\"Searching for images in catalog {}\".format(os.path.basename(col)))\n",
    "        with open(col) as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if (int(row[\"path\"]) == int(sat_path) and int(row[\"row\"]) == int(sat_row) and row['dayOrNight'] == 'DAY'):\n",
    "                    date_acq = parser.parse(row[\"acquisitionDate\"]).date()\n",
    "                    \n",
    "                    if (start < date_acq < end):\n",
    "                        files.append(row[\"sceneID\"])\n",
    "                        ##print(date_acq)\n",
    "    print(str(len(files)) + \" images have been found in \" + str(len(collection)) + \" collection(s)\")\n",
    "    return(files)\n",
    "\n",
    "#bottleneck!\n",
    "def generate_urls(sat_ids, destdir, sat, arc_col, usgs_details):\n",
    "    urls = []\n",
    "    \n",
    "    #pre_collection is not on the API's db so the USGS_dir has to be found manually, these work though for the time being:\n",
    "    if arc_col == 0:\n",
    "        if sat == \"LC8\":\n",
    "            usgs_dir = ['4923']\n",
    "        if sat == \"LE7\":\n",
    "            usgs_dir = ['3372', '3373']\n",
    "        if sat == \"LT5\":\n",
    "            usgs_dir = ['3119', '4345']\n",
    "    \n",
    "    if arc_col == 1:\n",
    "        urls = list(map(lambda satids, usgs : \"https://earthexplorer.usgs.gov/download/{}/{}/STANDARD/EE\".format(usgs, satids), sat_ids, usgs_get_dir(sat_ids, sat, usgs_details)))\n",
    "        print(str(len(urls)) + \" URLs have been generated, returning..\")\n",
    "        return(urls)\n",
    "\n",
    "    for product_id in sat_ids:        \n",
    "        #product_tgz = os.path.join(destdir, product_id + '.tgz')\n",
    "        #product_dest = os.path.join(destdir, product_id)\n",
    "\n",
    "        for usgs_path in usgs_dir: \n",
    "            url = \"https://earthexplorer.usgs.gov/download/{}/{}/STANDARD/EE\".format(usgs_path, product_id)\n",
    "            if requests.get(url).status_code == 200:\n",
    "                urls.append(url)\n",
    "                \n",
    "    if not len(urls) == len(sat_ids):\n",
    "        print(str(len(urls)) + \" URLs have been generated with possible duplicates..\")\n",
    "    else:\n",
    "        print(str(len(urls)) + \" URLs have been generated, returning..\")\n",
    "    return(urls)\n",
    "\n",
    "\n",
    "def download_file(url, destdir):\n",
    "    r = requests.get(url, stream = True)\n",
    "    with open(destdir, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return\n",
    "    \n",
    "def usgs_get_dir(ids, sat, usgs_details):\n",
    "    retries = 3\n",
    "    if sat == \"LC8\":\n",
    "        ls_dataset = 'LANDSAT_8_C1'\n",
    "    if sat == \"LE7\":\n",
    "        ls_dataset = 'LANDSAT_ETM_C1' #check differences in SLC!\n",
    "    if sat == \"LT5\":\n",
    "        ls_dataset = 'LANDSAT_TM_C1'\n",
    "    \n",
    "    #won't catch other non-USGS error (yet!)\n",
    "    for i in range(retries):    \n",
    "        try:\n",
    "            usgs_dirs = list(map(lambda x: x['metadataUrl'].split(\"/\")[5], api.metadata(ls_dataset, node = \"EE\", entityids=ids)['data']))\n",
    "            return(usgs_dirs)\n",
    "        except USGSError as u:\n",
    "            if u.args[0].find(\"API\") != -1:\n",
    "                print(\"Could not find API key, retrying..\")\n",
    "                usgs_login(usgs_details=usgs_details)\n",
    "            else:\n",
    "                print(repr(u))\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "def usgs_login(usgs_details):\n",
    "    api.login(usgs_details['username'], usgs_details['password'])\n",
    "    \n",
    "def manual_assessment(sat_ids, sat, usgs_details):\n",
    "    break_return = sat_ids[:]\n",
    "    retries = 3\n",
    "    or_len = len(sat_ids)\n",
    "    if sat == \"LC8\":\n",
    "        dataset = 'LANDSAT_8_C1'\n",
    "    elif sat == \"LE7\":\n",
    "        dataset = 'LANDSAT_ETM_C1' #check differences in SLC!\n",
    "    elif sat == \"LT5\":\n",
    "        dataset = 'LANDSAT_TM_C1'\n",
    "    else:\n",
    "        #print(str(sat) + \" is not found in API, exiting..\")\n",
    "        raise NotImplementedError(\"The specified satellite has not been found\")\n",
    "\n",
    "    \n",
    "    for i in range(retries):    \n",
    "        try:\n",
    "            previews = list(map(lambda x: x['browseUrl'], api.metadata(dataset=dataset, node = \"EE\", entityids=sat_ids)['data']))\n",
    "            for i in range(len(previews)):\n",
    "                response = requests.get(previews[i])\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img.show()\n",
    "                var = \"\"\n",
    "                while len(var) == 0:\n",
    "                    var = input((str(i + 1) + \"/\" + str(len(previews)) + \" Keep this image? y/n  \")).lower()\n",
    "                    if (var == \"y\" or var == \"yes\"):\n",
    "                        continue\n",
    "                    elif (var == \"n\" or var == \"no\"):\n",
    "                        sat_ids[i] = None\n",
    "                    elif (var == \"q\" or var == \"quit\" or var == \"quit()\"):\n",
    "                        print(\"Quitting, returning original input\")\n",
    "                        return(break_return)\n",
    "                    else:\n",
    "                        continue\n",
    "                img.close()\n",
    "\n",
    "            sat_ids = list(filter(lambda x: bool(x), sat_ids))\n",
    "            print(\"Returning \" + str(len(sat_ids)) + \"/\" + str(or_len) + \" ids\")\n",
    "            return(sat_ids)\n",
    "\n",
    "        except USGSError as u:\n",
    "            if u.args[0].find(\"API\") != -1:    \n",
    "                print(\"Could not find API key, retrying..\")\n",
    "                usgs_login(usgs_details=usgs_details)\n",
    "            else:\n",
    "                print(repr(u))\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "\n",
    "def downloader(urls, destdir, usgs_details):\n",
    "    for i in range(len(urls)):\n",
    "        #SLOW, use IO/requests!\n",
    "        product_id = urls[i].split(\"/\")[5]\n",
    "        product_tgz = os.path.join(destdir, product_id + '.tgz')\n",
    "        if os.path.isfile(product_tgz):\n",
    "            print(product_id + \" is already found, cont..\")\n",
    "            continue\n",
    "        \n",
    "        product_dest = os.path.join(destdir, product_id)\n",
    "        print(\"Downloading..  \" + str(i + 1) + \"/\" + str(len(urls)))\n",
    "\n",
    "        cookies = depr_urllib2.HTTPCookieProcessor()\n",
    "        opener = depr_urllib2.build_opener(cookies)\n",
    "        depr_urllib2.install_opener(opener)\n",
    "\n",
    "        data = str(depr_urllib2.urlopen(\"https://ers.cr.usgs.gov\").read())\n",
    "        m = re.search(r'<input .*?name=\"csrf_token\".*?value=\"(.*?)\"', data)\n",
    "        if m:\n",
    "            token = m.group(1)\n",
    "        else :\n",
    "            print(\"Error : CSRF_Token not found\")\n",
    "            sys.exit(-3)\n",
    "\n",
    "        params = urllib.parse.urlencode(dict(username=usgs_details['username'],password= usgs_details['password'], csrf_token=token)).encode(\"utf-8\")\n",
    "        request = urllib.request.Request(\"https://ers.cr.usgs.gov/login\", params, headers={})\n",
    "        f = depr_urllib2.urlopen(request)\n",
    "\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        # if data.find('You must sign in as a registered user to download data or place orders for USGS EROS products')>0 :\n",
    "        #     print(\"Authentification failed\")\n",
    "        #     sys.exit(-1)\n",
    "\n",
    "        try:\n",
    "            req = depr_urllib2.urlopen(urls[i])\n",
    "            downloaded = 0\n",
    "            CHUNK = 1024 * 1024 *64\n",
    "            with open(product_tgz, 'wb') as fp:\n",
    "                while True:\n",
    "                    chunk = req.read(CHUNK)\n",
    "                    downloaded += len(chunk)\n",
    "\n",
    "                    if not chunk: break\n",
    "                    fp.write(chunk)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(product_id + \" has been downloaded to: \" + destdir)\n",
    "\n",
    "def get_usgs_details(txt):\n",
    "    usgs_details = {}\n",
    "    with open(txt) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                a, b = line.split()\n",
    "                usgs_details['username'] = a\n",
    "                usgs_details['password'] = b\n",
    "            except:\n",
    "                pass\n",
    "    if not usgs_details['password']:\n",
    "        raise ValueError(\"error with .txt file\")        \n",
    "    return(usgs_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata already exists, checking if up-to-date..\n",
      "Metadata up-to-date for specified dates, cont..\n",
      "Searching for images in catalog LANDSAT_8_C1.csv\n",
      "21 images have been found in 1 collection(s)\n",
      "21 URLs have been generated, returning..\n"
     ]
    }
   ],
   "source": [
    "#example of usage follows:\n",
    "\n",
    "destdir = \"M:/My Documents/git/landsat-dl/ls_imagery/\"  \n",
    "sat_path = '198'\n",
    "sat_row = '23'\n",
    "sat = 'LC8' #LC8, LT5 or LE7 \n",
    "arc_col = 1 #or 0 without support for manual assessment   \n",
    "\n",
    "#if left blank, will default to start/end of specified satellite\n",
    "start = datetime.date(2014, 1, 1)\n",
    "end = datetime.date(2014, 12, 1)\n",
    "\n",
    "sat_ids = find_in_collection(start = start, end = end, sat_path = sat_path, sat_row=sat_row, sat = sat, destdir=destdir, arc_col=arc_col)\n",
    "#sat_ids = manual_assessment(sat_ids, sat, usgs_details) #uses the default image handler \n",
    "urls = generate_urls(sat_ids, destdir=destdir, sat = sat, arc_col=arc_col, usgs_details= usgs_details)\n",
    "#downloader(urls, destdir, usgs_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other databases:\n",
    "\n",
    "http://lsiexplorer.cr.usgs.gov/\n",
    "CWIC_LSI_EXPLORER_CATALOG_NODE = \"CWIC\"\n",
    "\n",
    "http://earthexplorer.usgs.gov/\n",
    "EARTH_EXPLORER_CATALOG_NODE = \"EE\"\n",
    "\n",
    "http://hddsexplorer.usgs.gov/\n",
    "HDDS_EXPLORER_CATALOG_NODE = \"HDDS\"\n",
    "\n",
    "http://lpvsexplorer.cr.usgs.gov/\n",
    "LPCS_EXPLORER_CATALOG_NODE = \"LPCS\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
